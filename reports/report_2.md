# Отчет №2 по работе над командным проектом по LLM

## Сведения о проекте

**Тема проекта:** AI-гид

**Ментор:** Василий Лаврентьев

**Команда:** DreamTeamHouse

**Участники команды:**
- Лукоянова Василиса
- Лившиц Лев
- Миллер Алина
- Попов Владимир
- Михайлов Владислав

**Репозиторий проекта**

Мы ведем командную работу в двух репозиториях:

research & development – репозиторий, посвященный исследованию предметной области, доступных данных, рынка и технологий, технологический решений

код для RAG-системы тоже содержится здесь
https://github.com/exxyyf/ai-adventure-guide-dth

prod – репозиторий, содержащий код актуального решения для деплоя тг-бота и запуска на виртуальной машине
https://github.com/Levliv/AI-travel-guide

# Архитектура пайплайна

Сервис представляет собой RAG-pipeline в качестве baseline решения, который состоит из:

1. Модуля генерации ответов **Generator**:

- Скрипт generator.py
- Используемая модель: Mistral через API (mistralai пакет). Задача: на основе уточнённого вопроса и контекста выдаёт текстовый ответ.
- Системный prompt: эксперт по путешествиям
- Пользовательский prompt: добавляет retrieved_chunks + вопрос
- Ограничение длины ответа через max_tokens

2. Модуля **Retriever** - осуществляет поиск контента, скрипт retriever.py.

- Возвращает список текстов для дальнейшей генерации ответа LLM.
- Находит k наиболее релевантных контекстов и передаёт их в Generator

3. Модуля **Preprocessing**, который отвечает за понимание сути вопроса:

- Принимает вопрос пользователя и, используя модель MISTRAL, понимает суть вопроса, намерение пользователя и переформулирует вопрос

4. **API / Backend** - обработка запроса, поиск по векторному индексу, генерация ответа.

5. **Telegram Bot (Frontend)** - Пользователь задаёт вопрос через Telegram. Бот получает сообщение и передаёт его в backend.

<img src="figures/rag_architecture.png" alt="difficulty level" width="400">

# Взаимодействие компонентов


1. Пользователь задаёт вопрос через Telegram. Бот получает сообщение и передаёт его в backend.
2. **Telegram Bot** получает сообщение и отправляет его на Backend API.
3. **Backend API* обрабатывает запрос: 
- Передаёт текст в **QuestionUnderstanding**, чтобы определить intent, entities и уточнённый запрос (clarified_query).
- Получает структурированный JSON с информацией о запросе.
4. **Retriever** получает clarified_query и ищет наиболее релевантные чанки в FAISS-индексе:
- С помощью Embedder текст запроса преобразуется в вектор.
- FAISS индекс возвращает топ-k ближайших векторов.
5. **Generator** (Mistral LLM) получает:
- Уточнённый запрос.
- Релевантные текстовые блоки из Retriever.
- Генерирует ответ, строго опираясь на найденные данные.
6. **Backend API** отправляет ответ обратно в Telegram Bot.
7 **Telegram Bot** отображает пользователю сформированный ответ.


# Результаты тестирования

Мы тестировали несколько аспектов нашего решения:

1. RAG-система, ответы по текстовым вопросам
2. Системы для ответа по фотографиям, чтобы выбрать лучшую
	1. Image Captioning с помощью API Pixtral
	2. Мультимодальный RAG с помощью SigLip

## Тестирование RAG-системы

todo

### Описание валидационной выборки

Для создания валидационной выборки мы сгенерировали 100 вопросов разного уровня сложности, опираясь на наш датасет, и добавили к ним ground truth с помощью Claude Sonnet 4.5 (ver_long) и Haiku 4.5 (ver_short). В выборку мы включили также off-topic вопросы, на которые система должна отвечать, что запрос нерелевантен теме туризма и путешествий.

<img src="figures/difficulty_level_cnt.png" alt="difficulty level" width="400">


<img src="figures/question_type_cnt.png" alt="question type" width="500">


<img src="figures/topic_cnt.png" alt="topic cnt" width="800">

EDA для валидационной выборки [здесь](https://github.com/exxyyf/ai-adventure-guide-dth/blob/main/baselines/validation_data_eda.ipynb)

### Описание пайплайна тестирования

todo

### Итоговые метрики

todo

## Тестирование определения достопримечательности по фото

Мы хотим предоставить пользователю возможность отправлять в систему запрос не только текстом, но и изображением.

Здесь ссылка на [технологический обзор](https://github.com/exxyyf/ai-adventure-guide-dth/blob/main/baselines/image_as_an_input_report.md) и [реализацию](https://github.com/exxyyf/ai-adventure-guide-dth/tree/main/baselines) двух бейслайн решений

Задачей тестирования было выбрать 1 из 2 решений для дальнейшего встраивания в проект.

Виды тестирования:

- оценка качества корректного распознавания достопримечательности
- скорость ответа

Ноутбук с тестированием [здесь](https://github.com/exxyyf/ai-adventure-guide-dth/blob/main/baselines/testing_image_input.ipynb)

Ноутбук с подведением результатов [здесь](https://github.com/exxyyf/ai-adventure-guide-dth/blob/main/baselines/testing_image_input_results.ipynb)

### Тестирование качества распознавания

Мы создали валидационную выборку из 20 изображений, 10 из которых относятся к широко известным достопримечательностям, а остальные 10 - к малоизвестным.

В случае 1 сценария, Pixtral дает текстовое описание картинки
В случае 2 сценария, по расстоянию между эмбеддингами картинки и текстов (сделанных SigLip), определяются чанки датасета с подходящей информацией

Суть оценки качества распознавания - это оценить:
- в 1 случае - правильно ли модель описывает достопримечательность
- во 2 случае - находится ли по созданным эмбеддингам подходящая статья из датасета, содержащая информацию про достопримечательность

Оценивали по разработанному [policy](https://github.com/exxyyf/ai-adventure-guide-dth/blob/main/baselines/labelling_policy_image_input_testing.md), задавали 4 возможных лейбла.

Оценивали `accuracy` трех видов:

- **strict_acc** - принимаем L0 как единственно верный ответ

- **relaxed_acc** - принимаем L0 и L1 как правильные ответы

- **semantic_acc** - принимаем L0, L1 и L2 как правильные ответы

Подробнее по лейблам можно почитать в [policy](https://github.com/exxyyf/ai-adventure-guide-dth/blob/main/baselines/labelling_policy_image_input_testing.md).

| model                          |   strict_acc |   relaxed_acc |   semantic_acc |
|:-------------------------------|-------------:|--------------:|---------------:|
| google/siglip-base-patch16-384 |         0.55 |          **0.9**  |           **0.95** |
| pixtral-12b-2409               |         **0.75** |          0.75 |           **0.95** |

Также проверили `accuracy` в разрезе сложности вопроса easy/hard:

easy:

| model                          |   strict_acc |   relaxed_acc |   semantic_acc |
|:-------------------------------|-------------:|--------------:|---------------:|
| google/siglip-base-patch16-384 |          0.8 |             **1** |              **1** |
| pixtral-12b-2409               |          **1**   |             **1** |              **1** |

hard:

| model                          |   strict_acc |   relaxed_acc |   semantic_acc |
|:-------------------------------|-------------:|--------------:|---------------:|
| google/siglip-base-patch16-384 |          0.3 |           **0.8** |            **0.9** |
| pixtral-12b-2409               |          **0.5** |           0.5 |            **0.9** |

### Тестирование скорости ответа

Для Pixtral скорость ответа - это время получение ответа по API (latency_total)

Для SigLip подсчитывалось несколько latency:

- latency_emb - время формирования моделью эмбеддинга из картинки
- latency_search - время нахождения релевантных текстовых чанков по эмбеддингу картинки
- latency_total = latency_emb + latency_search

В таблицах метрики по средней latency.

total latency:

| approach              |   latency_total |
|:----------------------|----------:|
| Pixtral               |  2.91905  |
| Image→RAG with SigLip |  **0.196891** |


siglip:
| model                          |   latency_emb |   latency_search |   latency_total |
|:-------------------------------|--------------:|-----------------:|----------------:|
| google/siglip-base-patch16-384 |      0.168105 |        0.0287856 |        0.196891 |


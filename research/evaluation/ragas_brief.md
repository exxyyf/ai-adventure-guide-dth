
### **Философия и принципы RAGAS**

RAGAS не требует эталонных (ground truth) ответов от человека. Он оценивает качество по трем ключевым аспектам, вычисляемым на основе:

1. **Вопрос** (`question`)
    
2. **Сгенерированный ответ** (`answer`)
    
3. **Использованные контексты/факты** (`contexts`)
    
4. **Идеальный/Эталонный ответ** (`ground_truth`) — _опционально, но сильно улучшает оценку

**Ключевые метрики:**

- **Faithfulness (Верность контексту)**: Насколько ответ основан на предоставленном контексте. Борется с "галлюцинациями".
    
- **Answer Relevance (Релевантность ответа)**: Насколько ответ прямо соответствует вопросу.
    
- **Context Relevance (Релевантность контекста)**: Насколько извлеченные контексты релевантны вопросу.
    
- **Context Recall (Полнота контекста)**: _Требует ground truth_. Насколько все ключевые факты из эталонного ответа присутствуют в контексте.

---

### **Пошаговый план оценки**

#### **Шаг 0: Предварительные условия**

Убедитесь, что у вас есть:

1. **Рабочая RAG-система** с четко выделенными компонентами: Retriever (поиск статей/фрагментов из Wikivoyage) и Generator (LLM, формирующая ответ).
    
2. **Набор тестовых вопросов** (~50-100 вопросов). Они должны покрывать разные типы запросов:
    
    - _Фактические_: "Какие главные достопримечательности в Барселоне?"
        
    - _Списковые_: "Назовите лучшие пляжи на Пхукете".
        
    - _Советующие_: "Какой район Рима выбрать для проживания с детьми?"
        
    - _Уточняющие_: "Нужна ли виза в Таиланд для граждан РФ на 30 дней?"
        
3. **Доступ к LLM** (например, через OpenAI, Anthropic, или локальная модель через Ollama) для вычисления метрик. RAGAS использует LLM как "судью".

#### **Шаг 1: Сбор датасета для оценки**

Это самый важный этап. Вам нужно создать датасет (`eval_dataset`) — список словарей, где каждый элемент содержит:

- `question`: ваш тестовый вопрос.
    
- `answer`: ответ, сгенерированный **вашей RAG-системой**.
    
- `contexts`: список текстовых фрагментов (например, 3-5 самых релевантных), которые ваша система **фактически передала в LLM** для генерации этого ответа.
    
- `ground_truth` (желательно): идеальный, полный ответ, который вы ожидаете. Его можно написать вручную, взять из другой авторитетной статьи или сгенерировать мощной LLM (например, GPT-4) на основе **полной статьи** Wikivoyage.

**Как собрать `contexts`?** Нужно модифицировать вашу RAG-систему так, чтобы она логировала не только итоговый ответ, но и те чанки/отрывки, которые Retriever передал в LLM.

**Итог Шага 1:** У вас есть файл `eval_dataset.jsonl` или pandas DataFrame со структурой:

```python

[
    {
        "question": "Какие главные достопримечательности в Барселоне?",
        "answer": "К основным достопримечательностям Барселоны относятся...",
        "contexts": ["[История Барселоны]...", "[Архитектура Гауди] Саграда Фамилия, парк Гуэль...", "[Культура]..."],
        "ground_truth": "Саграда Фамилия, парк Гуэль, Каса Бальо, Готический квартал..."
    },
    # ... остальные вопросы
]
```

#### **Шаг 2: Выбор и вычисление метрик**

Решите, что для вас важнее всего.

1. **Базовый набор (без `ground_truth`)**:
    
    - `faithfulness` — критично, чтобы ответ не врал.
        
    - `answer_relevance` — критично, чтобы ответ был по делу.
        
    - `context_relevance` — оценит качество работы Retriever'а.
        
2. **Расширенный набор (с `ground_truth`)**:
    
    - Добавляется `context_recall` — насколько хорошо Retriever находит ВСЕ нужные факты.
        
    - Также можно считать `answer_correctness` и `answer_similarity`.
    

**Пример кода на Python:**

```python
import pandas as pd
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevance, context_relevance, context_recall
from datasets import Dataset

# Загружаем датасет
df = pd.read_json('eval_dataset.jsonl', lines=True)
eval_dataset = Dataset.from_pandas(df)

# Выбираем метрики (пример с ground_truth)
metrics = [faithfulness, answer_relevance, context_relevance, context_recall]

# Запускаем оценку
# Необходимо задать LLM для оценки (например, gpt-3.5-turbo)
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-3.5-turbo")
# Для embeddings (нужны для некоторых метрик)
from langchain_openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()

result = evaluate(
    dataset=eval_dataset,
    metrics=metrics,
    llm=llm,
    embeddings=embeddings
)

# Смотрим результат
print(result)
df_result = result.to_pandas()
df_result.to_csv('ragas_evaluation_results.csv', index=False)
```

#### **Шаг 3: Анализ результатов и выводы**

1. **Посмотрите на агрегированные scores** (средние по датасету). Хорошие значения обычно > 0.8, удовлетворительные > 0.6.
    
2. **Проанализируйте примеры с низкими оценками** по каждой метрике:
    
    - **Низкий `faithfulness`**: Ищите галлюцинации. Проблема в LLM или в том, что в контексте нет ответа?
        
    - **Низкий `answer_relevance`**: Ответ расплывчатый или не по вопросу? Проблема в формулировке промпта или в качестве контекста?
        
    - **Низкий `context_relevance`**: Retriever приносит мусор. Нужно доработать чанкинг (разбиение статей), улучшить эмбеддинги или добавить переранжирование.
        
    - **Низкий `context_recall`**: Retriever находит часть фактов, но не все. Увеличьте количество извлекаемых чанков (`k`), улучшите поиск.
        
3. **Свяжите метрики с компонентами системы**:
    
    - `context_relevance` & `context_recall` — **качество Retriever**.
        
    - `faithfulness` & `answer_relevance` — **качество Generator + промптинга**.
    

#### **Шаг 4: Итеративное улучшение системы**

На основе анализа:

1. **Если проблема в Retriever**:
    
    - Поэкспериментируйте со способом разбиения текстов (размер чанка, перекрытие).
        
    - Попробуйте другие модели эмбеддингов (например, `text-embedding-3-small`).
        
    - Добавьте гибридный поиск (семантический + ключевые слова).
        
    - Настройте порог релевантности для фильтрации.
    
2. **Если проблема в Generator**:
    
    - Улучшите системный промпт, добавив строгие инструкции: "Отвечай ТОЛЬКО на основе предоставленного контекста".
        
    - Поэкспериментируйте с температурой генерации (ставите 0 для большей детерминированности).
        
    - Попробуйте более мощную LLM для генерации ответов.
        
3. **Пересоберите датасет** после изменений и **запустите оценку снова**. Сравните результаты с предыдущим прогоном.


### **Типичные подводные камни и советы**

1. **Качество `ground_truth`**: Если берете эталонный ответ из той же статьи Wikivoyage, но другим способом (не через ваш Retriever), метрика `context_recall` будет честной.
    
2. **Стоимость и время**: Оценка с использованием платных LLM (как OpenAI) может стоить денег и занимать время на больших датасетах. Начните с подмножества (20-30 вопросов).
    
3. **Ложные срабатывания**: LLM-"судья" не идеальна. Иногда она может ошибаться в оценке. Критически смотрите на низкие баллы.
    
4. **Дополнительные метрики RAGAS**: Рассмотрите `answer_correctness` (комбинация faithfulness и similarity с ground_truth) как интегральную метрику.
    
5. **Визуализация**: Используйте `matplotlib` или `seaborn` для построения графиков распределения оценок и корреляции между метриками.

### **Примерный итоговый пайплайн:**

Сбор тестовых вопросов -> Создание эталонных ответов -> Запуск RAG на вопросах + сбор контекстов -> Формирование eval_dataset -> Запуск evaluate() из RAGAS -> Анализ дашборда -> Внесение изменений в RAG -> Повторение цикла.